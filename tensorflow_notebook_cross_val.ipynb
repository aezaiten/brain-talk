{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "seed_num = 45\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_num)\n",
    "import random\n",
    "random.seed(seed_num)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_num)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import operator as op\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "dataset_folder = os.path.abspath(\"./individual_npzs_logmel/{0}/*.npz\")\n",
    "checkpoint_folder = \"acrnn_topology3_es25\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "batch_size = 50\n",
    "dropout = 0.1\n",
    "recurrent_dropout = 0.9\n",
    "final_dropout = 0.5\n",
    "max_pool = 2\n",
    "strides = 1\n",
    "input_size = 60000\n",
    "output_size = 4\n",
    "epochs = 500\n",
    "timesteps = 75\n",
    "\n",
    "first_maps = 32\n",
    "second_maps = 48\n",
    "third_maps = 64\n",
    "\n",
    "first_kernel = [3, 5, 1, first_maps]\n",
    "second_kernel = [3, 5, first_maps, second_maps]\n",
    "third_kernel = [3, 5, second_maps, third_maps]\n",
    "\n",
    "hidden_layer = 256\n",
    "dense_layer = 64\n",
    "\n",
    "early_stop = 25\n",
    "\n",
    "label_dictionary = {'ang': 0, 'hap': 1, 'neu': 2, 'sad': 3}\n",
    "onehot_dictionary = {0: 'ang', 1: 'hap', 2:'neu', 3:'sad'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, labels):\n",
    "    steps = math.ceil(data.shape[0] / batch_size)\n",
    "    for batch_step in range(0, steps):\n",
    "        start = batch_size * batch_step\n",
    "        end = batch_size * (batch_step + 1)\n",
    "        yield data[start:end], labels[start:end], batch_step\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    return np.ndarray.tolist(class_weight.compute_class_weight('balanced', np.unique(labels), labels))        \n",
    "\n",
    "def build_encoded_array(emotion_label):\n",
    "    initialized_array = [0. for key in label_dictionary]\n",
    "    initialized_array[label_dictionary[emotion_label]] = 1.\n",
    "    return initialized_array\n",
    "        \n",
    "def onehot_encode(label_minibatch):\n",
    "    return [build_encoded_array(emotion_label) for emotion_label in label_minibatch]\n",
    "\n",
    "def onehot_decode(onehot_labels):\n",
    "    return [onehot_dictionary[label.index(1.0)] for label in onehot_labels]\n",
    "\n",
    "def weighted_accuracy(y_pred, y_true):\n",
    "    #Suma vsetkych spravne urcenych viet predelena vsetkymi vetami\n",
    "\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    for key in y_true:\n",
    "        correct += y_pred[key]\n",
    "        all += y_true[key]\n",
    "\n",
    "    return correct/all\n",
    "\n",
    "\n",
    "def unweighted_accuracy(y_pred, y_true):\n",
    "\n",
    "    #Suma poctu spravnych viet lomeno vsetky vety pre kazdu emociu (triedu) predelena poctom tried\n",
    "\n",
    "    correct = 0\n",
    "    emotions = len(y_true)\n",
    "    for key in y_true:\n",
    "        correct += y_pred[key]/y_true[key]\n",
    "\n",
    "    return correct/emotions\n",
    "\n",
    "def validation_results(current_sess, data, labels):\n",
    "    predictions = current_sess.run(correct_prediction, feed_dict={X:data, Y:labels, keep_prob:1.0, normalization_switch: False})\n",
    "    coupled_validation = list(zip(predictions, onehot_decode(labels)))\n",
    "    final_dict = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "    overall_dict = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "\n",
    "    for couple in coupled_validation:\n",
    "        if (couple[0]):\n",
    "            final_dict[couple[1]] += 1\n",
    "        overall_dict[couple[1]] += 1\n",
    "    unweighted_acc = unweighted_accuracy(final_dict, overall_dict)\n",
    "    return unweighted_acc\n",
    "\n",
    "def compute_mean_std(data):\n",
    "    dataset = []\n",
    "    for key in data:\n",
    "        dataset.extend(data[key]['M'][0])\n",
    "        dataset.extend(data[key]['F'][0])\n",
    "    \n",
    "    dataset = np.asarray(dataset)\n",
    "    data_to_norm = np.zeros([len(dataset), dataset[0].shape[0], dataset[0].shape[1]], dtype=np.float32)\n",
    "    for spect_data in range(len(dataset)):\n",
    "        data_to_norm[spect_data,:,:] = dataset[spect_data]\n",
    "        \n",
    "    dataset_mean = np.ndarray.astype(np.mean(data_to_norm, axis=(0,1,2), dtype='float64', keepdims=True), dtype='float32')\n",
    "    dataset_std = np.ndarray.astype(np.std(data_to_norm, axis=(0,1,2), dtype='float64', keepdims=True), dtype='float32')\n",
    "\n",
    "    return dataset_mean, dataset_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset: 4 sessions for training, 1 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(data, to_skip, mean, std):\n",
    "    \n",
    "    # load for train except validation and train session\n",
    "    train_dataset = []\n",
    "    train_labels = []\n",
    "    for key in data:\n",
    "        if key != to_skip:\n",
    "            train_dataset.extend(data[key]['M'][0])\n",
    "            train_dataset.extend(data[key]['F'][0])\n",
    "            train_labels.extend(data[key]['M'][1])\n",
    "            train_labels.extend(data[key]['F'][1])\n",
    "    \n",
    "    # transform to desired shapes\n",
    "    train_dataset = np.asarray(train_dataset)\n",
    "    train_data = np.zeros([len(train_dataset), train_dataset[0].shape[0], train_dataset[0].shape[1]], dtype=np.float32)\n",
    "    for spect_data in range(len(train_dataset)):\n",
    "        train_data[spect_data,:,:] = train_dataset[spect_data]\n",
    "    \n",
    "    other_dataset = np.concatenate((np.asarray(data[to_skip]['M'][0]), np.asarray(data[to_skip]['F'][0])))\n",
    "    other_labs = np.concatenate((np.asarray(data[to_skip]['M'][1]), np.asarray(data[to_skip]['F'][1])))\n",
    "    other_data = np.zeros([len(other_dataset), other_dataset[0].shape[0], other_dataset[0].shape[1]], dtype=np.float32)\n",
    "    for spect_data in range(len(other_dataset)):\n",
    "        other_data[spect_data,:,:] = other_dataset[spect_data]\n",
    "        \n",
    "    # z-norm the dataset\n",
    "    train_data = (train_data - mean) / std\n",
    "        \n",
    "    # shuffle the dataset\n",
    "    train_data, train_labels = shuffle(train_data, train_labels, random_state=seed_num)\n",
    "    train_data = train_data.reshape((train_data.shape[0], input_size))\n",
    "\n",
    "    # compute class_weights and onehot encode after\n",
    "    class_weights = compute_class_weights(train_labels)\n",
    "    train_labels = onehot_encode(train_labels)\n",
    "    \n",
    "    return train_data, train_labels, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_test_sets(data, gender, test_val_session, mean, std):\n",
    "    \n",
    "    test_dataset = data[test_val_session][gender][0]\n",
    "    test_labels = data[test_val_session][gender][1]\n",
    "    \n",
    "    validation_dataset = []\n",
    "    validation_labels = []\n",
    "    \n",
    "    if gender == 'M':\n",
    "        validation_dataset = data[test_val_session]['F'][0]\n",
    "        validation_labels = data[test_val_session]['F'][1]\n",
    "    else:\n",
    "        validation_dataset = data[test_val_session]['M'][0]\n",
    "        validation_labels = data[test_val_session]['M'][1]\n",
    "    \n",
    "    # transform to desired shapes\n",
    "    test_dataset = np.asarray(test_dataset)\n",
    "    test_data = np.zeros([len(test_dataset), test_dataset[0].shape[0], test_dataset[0].shape[1]], dtype=np.float32)\n",
    "    for spect_data in range(len(test_dataset)):\n",
    "        test_data[spect_data,:,:] = test_dataset[spect_data]\n",
    "    \n",
    "    validation_dataset = np.asarray(validation_dataset)\n",
    "    validation_data = np.zeros([len(validation_dataset), validation_dataset[0].shape[0], validation_dataset[0].shape[1]], dtype=np.float32)\n",
    "    for spect_data in range(len(validation_dataset)):\n",
    "        validation_data[spect_data,:,:] = validation_dataset[spect_data]\n",
    "        \n",
    "    # z-norm the dataset\n",
    "    validation_data = (validation_data - mean) / std\n",
    "    test_data = (test_data - mean) / std\n",
    "\n",
    "    test_data = test_data.reshape((test_data.shape[0], input_size))\n",
    "    validation_data = validation_data.reshape((validation_data.shape[0], input_size))\n",
    "\n",
    "    # onehot encode labels\n",
    "    test_labels = onehot_encode(test_labels)\n",
    "    validation_labels = onehot_encode(validation_labels)\n",
    "    \n",
    "    return test_data, test_labels, validation_data, validation_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(to_process, weights, biases, batchnorm_switch, strides=1):\n",
    "    conv_out = tf.nn.conv2d(to_process, weights, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    bias_out = tf.nn.bias_add(conv_out, biases)\n",
    "    batchnorm_out = tf.layers.batch_normalization(bias_out, training=batchnorm_switch)\n",
    "    relu_out = tf.nn.leaky_relu(batchnorm_out, 0.01)\n",
    "    return relu_out\n",
    "\n",
    "def maxpool2d(to_pool, pool_size=2):\n",
    "    maxpool_out = tf.nn.max_pool(to_pool, ksize=[1, pool_size, pool_size, 1], strides=[1, pool_size, pool_size, 1], padding='SAME')\n",
    "    return maxpool_out\n",
    "\n",
    "def attention(rnn_inputs, attention_size):\n",
    "\n",
    "    hidden_size = rnn_inputs.shape[2].value\n",
    "\n",
    "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    v = tf.sigmoid(tf.tensordot(rnn_inputs, W_omega, axes=1) + b_omega)\n",
    "    vu = tf.tensordot(v, u_omega, axes=1) \n",
    "    alphas = tf.nn.softmax(vu)          \n",
    "\n",
    "    output = tf.reduce_sum(rnn_inputs * tf.expand_dims(alphas, -1), 1)\n",
    "    \n",
    "    return output\n",
    "   \n",
    "\n",
    "def nn_pipeline_rnn(spectrogram, weights, biases, dropout_num, rnn_dropout, fully_connected_dropout, training_switch):\n",
    "    reshaped_input = tf.reshape(spectrogram, shape=[-1, 200, 300, 1])\n",
    "\n",
    "    first_layer_out = conv2d(reshaped_input, weights['first_layer_weights'], biases['first_layer_biases'], normalization_switch)\n",
    "    first_drop_out = tf.layers.dropout(first_layer_out, dropout_num, seed=seed_num+256, training=training_switch)\n",
    "    first_maxpool_out = maxpool2d(first_drop_out, pool_size=2)\n",
    "\n",
    "    second_layer_out = conv2d(first_maxpool_out, weights['second_layer_weights'], biases['second_layer_biases'], normalization_switch)\n",
    "    second_drop_out = tf.layers.dropout(second_layer_out, dropout_num, seed=seed_num+128, training=training_switch)\n",
    "    second_maxpool_out = maxpool2d(second_drop_out, pool_size=2)\n",
    "\n",
    "    third_layer_out = conv2d(second_maxpool_out, weights['third_layer_weights'], biases['third_layer_biases'], normalization_switch)\n",
    "    third_drop_out = tf.layers.dropout(third_layer_out, dropout_num, seed=seed_num+1024, training=training_switch)\n",
    "    #third_maxpool_out = maxpool2d(third_drop_out, pool_size=2)\n",
    "    \n",
    "    interim_shape = third_drop_out.get_shape().as_list()\n",
    "    transposed = tf.transpose(third_drop_out, perm=[0, 2, 1, 3])\n",
    "    reshape_for_rnn = tf.reshape(transposed, [-1, interim_shape[2], interim_shape[1]*interim_shape[3]])\n",
    "    reshape_for_rnn.set_shape([None, interim_shape[2], interim_shape[1]*interim_shape[3]])\n",
    "    \n",
    "    fw_cell = tf.contrib.rnn.GRUCell(hidden_layer)\n",
    "    bw_cell = tf.contrib.rnn.GRUCell(hidden_layer)\n",
    "    gru_fw_cell = tf.contrib.rnn.DropoutWrapper(cell=fw_cell, output_keep_prob=rnn_dropout, seed=(seed_num+32))\n",
    "    gru_bw_cell = tf.contrib.rnn.DropoutWrapper(cell=bw_cell, output_keep_prob=rnn_dropout, seed=(seed_num+64))\n",
    "    \n",
    "    gru_output, _ = tf.nn.bidirectional_dynamic_rnn(gru_fw_cell, gru_bw_cell, reshape_for_rnn, dtype=tf.float32)\n",
    "    gru_output_concatted = tf.concat(gru_output, axis=2)\n",
    "    \n",
    "    attention_out = attention(gru_output_concatted, 1)\n",
    "    \n",
    "    #interim_shape_gru = tf.shape(gru_output_concatted)\n",
    "    #gru_flatten = tf.reshape(gru_output_concatted, [-1, interim_shape_gru[1]*interim_shape_gru[2]])\n",
    "    #gru_dropout = tf.nn.dropout(gru_flatten, rnn_dropout, seed=seed_num+512)\n",
    "    \n",
    "    fully_connected_out = tf.add(tf.matmul(attention_out, weights['gru_weights']), biases['gru_biases'])\n",
    "    fully_connected_activation = tf.nn.leaky_relu(fully_connected_out, 0.01)\n",
    "    \n",
    "    prediction = tf.add(tf.matmul(fully_connected_activation, weights['output']), biases['output'])\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_rnn = {\n",
    "    'first_layer_weights': tf.Variable(tf.truncated_normal(first_kernel, stddev=0.1, seed=seed_num)),\n",
    "    'second_layer_weights': tf.Variable(tf.truncated_normal(second_kernel, stddev=0.1, seed=(seed_num+4))),\n",
    "    'third_layer_weights': tf.Variable(tf.truncated_normal(third_kernel, stddev=0.1,seed=(seed_num+8))),\n",
    "    'gru_weights': tf.Variable(tf.truncated_normal([2*hidden_layer, dense_layer], stddev=0.1, seed=(seed_num+2))),\n",
    "    'output': tf.Variable(tf.truncated_normal([dense_layer, output_size], stddev=0.1, seed=(seed_num+16)))\n",
    "}\n",
    "\n",
    "biases_rnn = {\n",
    "    'first_layer_biases': tf.get_variable('first_bias', [first_maps], initializer=tf.constant_initializer(0.1)),\n",
    "    'second_layer_biases': tf.get_variable('second_bias', [second_maps], initializer=tf.constant_initializer(0.1)),\n",
    "    'third_layer_biases': tf.get_variable('third_bias', [third_maps], initializer=tf.constant_initializer(0.1)),\n",
    "    'gru_biases': tf.get_variable('gru_bias', [dense_layer], initializer=tf.constant_initializer(0.1)),\n",
    "    'output': tf.get_variable('output_bias', [output_size], initializer=tf.constant_initializer(0.1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_network(session, val_data, val_labels, epoch_step, test_num, test_gender):\n",
    "\n",
    "    val_generator = batch_generator(val_data, val_labels)\n",
    "    acc_val_all = []\n",
    "    loss_val_all = []\n",
    "    predictions_all = np.asarray([])\n",
    "    for minibatch, labels, current_index in val_generator:\n",
    "        loss, acc = sess.run([loss_function, accuracy], feed_dict={X: minibatch, Y: labels, keep_prob: 1.0, keep_rnn_prob: 1.0, keep_fully_prob: 1.0, normalization_switch: False})\n",
    "        predics = sess.run(tf.argmax(prediction, 1), feed_dict={X: minibatch, Y: labels, keep_prob: 1.0, keep_rnn_prob: 1.0, keep_fully_prob: 1.0, normalization_switch: False})\n",
    "        predictions_all = np.concatenate((predictions_all, predics))\n",
    "        acc_val_all.append(acc)\n",
    "        loss_val_all.append(loss)\n",
    "    val_acc = np.mean(acc_val_all)\n",
    "    val_loss = np.mean(loss_val_all)\n",
    "    validation_confusion = confusion_matrix(predictions_all, [onehot.index(1.0) for onehot in validation_labels])\n",
    "    validation_recall = np.mean(np.diag(validation_confusion) / np.sum(validation_confusion, axis=1))\n",
    "    validation_unweighted = np.mean(np.diag(validation_confusion) / np.sum(validation_confusion, axis=0))\n",
    "    \n",
    "    print(\"Validation after epoch #\" + str(epoch_step) + \", Validation Loss= \"+ \"{:.4f}\".format(val_loss) + \", Validation Accuracy= \" + \"{:.3f}\".format(val_acc) + \", Unweighted Accuracy= \" + \"{:.3f}\".format(validation_unweighted) + \", Recall= \" + \" {:.3f}\".format(validation_recall))\n",
    "    print(validation_confusion)\n",
    "\n",
    "    acc_loss_file_validation = open(checkpoint_folder + \"val_loss_logs_session{0}_{1}.txt\".format(test_num, test_gender), \"a+\")\n",
    "    acc_loss_file_validation.write(\"{0}, {1}, {2}, {3}, {4}\\n\".format(epoch_step, val_acc, val_loss, validation_unweighted, validation_recall))\n",
    "    acc_loss_file_validation.close()\n",
    "    return val_loss, val_acc, validation_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(session, tr_data, tr_labels, val_data, val_labels, test_num, test_gender):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_loss = sys.maxsize\n",
    "    best_train_acc = 0\n",
    "    current_tolerance = 0\n",
    "    best_val_acc = 0\n",
    "    tolerance = 0\n",
    "    best_val_loss = sys.maxsize\n",
    "    best_val_unweighted = 0\n",
    "    \n",
    "    for epoch_step in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(tr_data, tr_labels)\n",
    "        train_acc_all = []\n",
    "        train_loss_all = []\n",
    "        for data_minibatch, label_minibatch, current_index in batch_gen:\n",
    "            session.run(train_trigger, feed_dict={X: data_minibatch, Y: label_minibatch, keep_prob: dropout, keep_rnn_prob: recurrent_dropout, keep_fully_prob:final_dropout, normalization_switch: True})\n",
    "            train_loss, train_acc = session.run([loss_function, accuracy], feed_dict={X: data_minibatch, Y: label_minibatch, keep_prob: 1.0, keep_rnn_prob: 1.0, keep_fully_prob: 1.0, normalization_switch: False})\n",
    "            train_acc_all.append(train_acc)\n",
    "            train_loss_all.append(train_loss)\n",
    "        train_acc_whole = np.mean(train_acc_all)\n",
    "        train_loss_whole = np.mean(train_loss_all)\n",
    "        acc_loss_file = open(checkpoint_folder + \"train_loss_logs_session{0}_{1}.txt\".format(test_num,test_gender), \"a+\")\n",
    "        acc_loss_file.write(\"{0}, {1}, {2}\\n\".format(epoch_step, train_acc_whole, train_loss_whole))\n",
    "        acc_loss_file.close()\n",
    "        print(\"\\nTraining accuracy and loss of epoch #\" + str(epoch_step) + \": {:.4f}\".format(train_acc_whole) + \", {:.4f}\".format(train_loss_whole))\n",
    "        \n",
    "        val_loss, val_acc, val_unweighted = validate_network(session, val_data, val_labels, epoch_step, test_num, test_gender)\n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            saver.save(sess, checkpoint_folder + \"loss_session{0}_{1}/best_model_loss.ckpt\".format(test_num, test_gender))\n",
    "            print(\"Saved model by validation loss {0}\".format(val_loss))\n",
    "            if tolerance != 0:\n",
    "                tolerance -= 1\n",
    "\n",
    "        if best_val_unweighted < val_unweighted:\n",
    "            best_val_unweighted = val_unweighted\n",
    "            saver.save(sess, checkpoint_folder + \"uacc_session{0}_{1}/best_model_uacc.ckpt\".format(test_num, test_gender))\n",
    "            print(\"Saved model by validation unweighted acc {0}\".format(val_unweighted))\n",
    "            tolerance = 0\n",
    "        else:\n",
    "            tolerance += 1\n",
    "        \n",
    "        if tolerance >= early_stop:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sess, npz_file, mean, std):\n",
    "    correct_label = npz_file['labels'][0]\n",
    "    final_prediction = []\n",
    "    prediction_add = [0, 0, 0, 0]\n",
    "    amount = 0\n",
    "    for spectrogram in npz_file['spectrograms']:\n",
    "        amount += 1\n",
    "        to_test = np.asarray(spectrogram)\n",
    "        to_test = (to_test - mean) / std\n",
    "        to_test = to_test.reshape(1, 60000)\n",
    "        \n",
    "        predic = sess.run(prediction, feed_dict={X: to_test, Y: onehot_encode([correct_label]), keep_prob: 1.0, keep_rnn_prob: 1.0, keep_fully_prob: 1.0, normalization_switch: False})\n",
    "        prediction_add = prediction_add + predic\n",
    "    prediction_add = prediction_add[0].tolist()\n",
    "    prediction_add = [pred/amount for pred in prediction_add]\n",
    "    final_prediction = prediction_add.index(max(prediction_add))\n",
    "    encoded = onehot_encode([correct_label])\n",
    "    correct_pred = encoded[0].index(1.0)\n",
    "    return final_prediction, correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(session, session_number, session_gender):\n",
    "    predicted_labels = []\n",
    "    correct_labels = []\n",
    "    sentence_amount = 0\n",
    "\n",
    "    for spectrogram in glob.glob(dataset_folder.format(\"session{0}\".format(session_number))):\n",
    "        gender = spectrogram.split('\\\\')[-1].split('_')[2][0]\n",
    "        if gender == session_gender:\n",
    "            sentence = np.load(spectrogram)\n",
    "            final, correct = test_sentence(sess, sentence, dmean, dstd)\n",
    "            predicted_labels.append(final)\n",
    "            correct_labels.append(correct)\n",
    "            sentence_amount += 1\n",
    "    matrix = confusion_matrix(predicted_labels, correct_labels)\n",
    "    test_recall = np.mean(np.diag(matrix) / np.sum(matrix, axis=1))\n",
    "    test_unweighted = np.mean(np.diag(matrix) / np.sum(matrix, axis=0))\n",
    "    acc = np.sum(np.diag(matrix)) / sentence_amount\n",
    "    percentage_matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "    print(\"Acc: {0}, UAcc: {1}, Recall: {2}\".format(acc, test_unweighted, test_recall))\n",
    "    print(matrix)\n",
    "    test_log_file = open(checkpoint_folder + \"test_logs_session{0}_{1}.txt\".format(session_number, session_gender), \"a+\")\n",
    "    test_log_file.write(\"{0}, {1}, {2}\\n\".format(acc, test_unweighted, test_recall))\n",
    "    test_log_file.write(\"{0}\\n\".format(matrix))\n",
    "    test_log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_results(session, session_number, session_gender):\n",
    "    predicted_labels = []\n",
    "    correct_labels = []\n",
    "    sentence_amount = 0\n",
    "\n",
    "    for spectrogram in glob.glob(dataset_folder.format(\"session{0}\".format(session_number))):\n",
    "        gender = spectrogram.split('\\\\')[-1].split('_')[2][0]\n",
    "        if gender == session_gender:\n",
    "            sentence = np.load(spectrogram)\n",
    "            final, correct = test_sentence(sess, sentence, dmean, dstd)\n",
    "            predicted_labels.append(final)\n",
    "            correct_labels.append(correct)\n",
    "            sentence_amount += 1\n",
    "    matrix = confusion_matrix(predicted_labels, correct_labels)\n",
    "    test_recall = np.mean(np.diag(matrix) / np.sum(matrix, axis=1))\n",
    "    test_unweighted = np.mean(np.diag(matrix) / np.sum(matrix, axis=0))\n",
    "    acc = np.sum(np.diag(matrix)) / sentence_amount\n",
    "    percentage_matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "    print(\"Acc: {0}, UAcc: {1}, Recall: {2}\".format(acc, test_unweighted, test_recall))\n",
    "    print(matrix)\n",
    "    \n",
    "    return acc, test_unweighted, test_recall, matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speakers = {1: {'M': (), 'F': ()},\n",
    "               2: {'M': (), 'F': ()},\n",
    "               3: {'M': (), 'F': ()},\n",
    "               4: {'M': (), 'F': ()},\n",
    "               5: {'M': (), 'F': ()}}\n",
    "\n",
    "session_string = 'session{0}'\n",
    "\n",
    "for i in range(1, 6):\n",
    "    male_spect = []\n",
    "    female_spect = []\n",
    "    male_labels = []\n",
    "    female_labels = []\n",
    "    formatted = session_string.format(i)\n",
    "    for spectrogram in glob.glob(dataset_folder.format(formatted)):\n",
    "        loaded_spec = np.load(spectrogram)\n",
    "        gender = spectrogram.split('\\\\')[-1].split('_')[2][0]\n",
    "        for spect in loaded_spec['spectrograms']:\n",
    "            if gender == 'M':\n",
    "                male_spect.append(spect)\n",
    "            else:\n",
    "                female_spect.append(spect)\n",
    "        for label in loaded_spec['labels']:\n",
    "            if gender == 'M':\n",
    "                male_labels.append(label)\n",
    "            else:\n",
    "                female_labels.append(label)\n",
    "    all_speakers[i] = {'M': (male_spect, male_labels), 'F': (female_spect, female_labels)}\n",
    "\n",
    "dmean, dstd = compute_mean_std(all_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session initialized with session 1 and test speaker M.\n",
      "\n",
      "Training accuracy and loss of epoch #1: 0.3557, 1.3548\n",
      "Validation after epoch #1, Validation Loss= 1.2592, Validation Accuracy= 0.316, Unweighted Accuracy= 0.327, Recall=  0.331\n",
      "[[ 67  30 108  48]\n",
      " [  1   1  17  17]\n",
      " [  1   0  51  44]\n",
      " [  0   0   2   2]]\n",
      "Saved model by validation loss 1.2591726779937744\n",
      "Saved model by validation unweighted acc 0.32695185730508863\n",
      "\n",
      "Training accuracy and loss of epoch #2: 0.3168, 1.2869\n",
      "Validation after epoch #2, Validation Loss= 1.2101, Validation Accuracy= 0.379, Unweighted Accuracy= 0.439, Recall=  0.375\n",
      "[[65 20 74 28]\n",
      " [ 2  6 13 16]\n",
      " [ 2  5 24 13]\n",
      " [ 0  0 67 54]]\n",
      "Saved model by validation loss 1.210054636001587\n",
      "Saved model by validation unweighted acc 0.43922382994116604\n",
      "\n",
      "Training accuracy and loss of epoch #3: 0.4163, 1.2369\n",
      "Validation after epoch #3, Validation Loss= 1.1655, Validation Accuracy= 0.426, Unweighted Accuracy= 0.473, Recall=  0.418\n",
      "[[61 18 60 21]\n",
      " [ 3  7  8 12]\n",
      " [ 5  6 35 13]\n",
      " [ 0  0 75 65]]\n",
      "Saved model by validation loss 1.165464162826538\n",
      "Saved model by validation unweighted acc 0.4730198054240319\n",
      "\n",
      "Training accuracy and loss of epoch #4: 0.4605, 1.1841\n",
      "Validation after epoch #4, Validation Loss= 1.1192, Validation Accuracy= 0.451, Unweighted Accuracy= 0.481, Recall=  0.415\n",
      "[[61 18 51 12]\n",
      " [ 3  5 11  9]\n",
      " [ 5  8 39 17]\n",
      " [ 0  0 77 73]]\n",
      "Saved model by validation loss 1.1191930770874023\n",
      "Saved model by validation unweighted acc 0.48052676871207534\n",
      "\n",
      "Training accuracy and loss of epoch #5: 0.4942, 1.1268\n",
      "Validation after epoch #5, Validation Loss= 1.0726, Validation Accuracy= 0.460, Unweighted Accuracy= 0.494, Recall=  0.426\n",
      "[[60 17 45 10]\n",
      " [ 1  6 14  9]\n",
      " [ 8  8 39 15]\n",
      " [ 0  0 80 77]]\n",
      "Saved model by validation loss 1.072613000869751\n",
      "Saved model by validation unweighted acc 0.49397710544431944\n",
      "\n",
      "Training accuracy and loss of epoch #6: 0.5294, 1.0708\n",
      "Validation after epoch #6, Validation Loss= 1.0398, Validation Accuracy= 0.490, Unweighted Accuracy= 0.530, Recall=  0.471\n",
      "[[59 14 41  8]\n",
      " [ 2  8 11  6]\n",
      " [ 8  8 41 11]\n",
      " [ 0  1 85 86]]\n",
      "Saved model by validation loss 1.0398080348968506\n",
      "Saved model by validation unweighted acc 0.5295622083309021\n",
      "\n",
      "Training accuracy and loss of epoch #7: 0.5546, 1.0236\n",
      "Validation after epoch #7, Validation Loss= 1.0277, Validation Accuracy= 0.497, Unweighted Accuracy= 0.542, Recall=  0.480\n",
      "[[58 13 36  5]\n",
      " [ 3  9 13  6]\n",
      " [ 7  8 40 10]\n",
      " [ 1  1 89 90]]\n",
      "Saved model by validation loss 1.0276684761047363\n",
      "Saved model by validation unweighted acc 0.5416080506811238\n",
      "\n",
      "Training accuracy and loss of epoch #8: 0.5750, 0.9906\n",
      "Validation after epoch #8, Validation Loss= 1.0281, Validation Accuracy= 0.507, Unweighted Accuracy= 0.548, Recall=  0.498\n",
      "[[57 11 32  4]\n",
      " [ 2  9 10  4]\n",
      " [ 9 10 42 10]\n",
      " [ 1  1 94 93]]\n",
      "Saved model by validation unweighted acc 0.5475506077961284\n",
      "\n",
      "Training accuracy and loss of epoch #9: 0.5914, 0.9679\n",
      "Validation after epoch #9, Validation Loss= 1.0339, Validation Accuracy= 0.502, Unweighted Accuracy= 0.561, Recall=  0.507\n",
      "[[ 55   8  27   4]\n",
      " [  3  12  11   4]\n",
      " [ 10  10  38   9]\n",
      " [  1   1 102  94]]\n",
      "Saved model by validation unweighted acc 0.5611320540957934\n",
      "\n",
      "Training accuracy and loss of epoch #10: 0.5999, 0.9513\n",
      "Validation after epoch #10, Validation Loss= 1.0366, Validation Accuracy= 0.494, Unweighted Accuracy= 0.545, Recall=  0.491\n",
      "[[ 55   8  23   4]\n",
      " [  3  10  13   3]\n",
      " [ 10  12  35   8]\n",
      " [  1   1 107  96]]\n",
      "\n",
      "Training accuracy and loss of epoch #11: 0.6098, 0.9365\n",
      "Validation after epoch #11, Validation Loss= 1.0312, Validation Accuracy= 0.509, Unweighted Accuracy= 0.561, Recall=  0.509\n",
      "[[ 55   8  22   4]\n",
      " [  3  11  14   3]\n",
      " [ 10  11  39   7]\n",
      " [  1   1 103  97]]\n",
      "Saved model by validation unweighted acc 0.5612287891055403\n",
      "\n",
      "Training accuracy and loss of epoch #12: 0.6170, 0.9217\n",
      "Validation after epoch #12, Validation Loss= 1.0239, Validation Accuracy= 0.509, Unweighted Accuracy= 0.559, Recall=  0.506\n",
      "[[ 54   8  21   4]\n",
      " [  4  11  15   3]\n",
      " [ 10  11  40   7]\n",
      " [  1   1 102  97]]\n",
      "Saved model by validation loss 1.0239325761795044\n",
      "\n",
      "Training accuracy and loss of epoch #13: 0.6277, 0.9078\n",
      "Validation after epoch #13, Validation Loss= 1.0184, Validation Accuracy= 0.512, Unweighted Accuracy= 0.558, Recall=  0.503\n",
      "[[53  8 20  4]\n",
      " [ 4 11 18  3]\n",
      " [11 11 42  7]\n",
      " [ 1  1 98 97]]\n",
      "Saved model by validation loss 1.0183911323547363\n",
      "\n",
      "Training accuracy and loss of epoch #14: 0.6312, 0.8937\n",
      "Validation after epoch #14, Validation Loss= 1.0143, Validation Accuracy= 0.510, Unweighted Accuracy= 0.557, Recall=  0.500\n",
      "[[53  8 20  4]\n",
      " [ 4 11 19  3]\n",
      " [11 11 41  7]\n",
      " [ 1  1 98 97]]\n",
      "Saved model by validation loss 1.0143344402313232\n",
      "\n",
      "Training accuracy and loss of epoch #15: 0.6351, 0.8805\n",
      "Validation after epoch #15, Validation Loss= 1.0128, Validation Accuracy= 0.510, Unweighted Accuracy= 0.557, Recall=  0.500\n",
      "[[53  8 19  4]\n",
      " [ 4 11 20  3]\n",
      " [11 11 41  7]\n",
      " [ 1  1 98 97]]\n",
      "Saved model by validation loss 1.0127619504928589\n",
      "\n",
      "Training accuracy and loss of epoch #16: 0.6392, 0.8687\n",
      "Validation after epoch #16, Validation Loss= 1.0017, Validation Accuracy= 0.510, Unweighted Accuracy= 0.550, Recall=  0.495\n",
      "[[53  9 20  4]\n",
      " [ 3 10 19  3]\n",
      " [12 11 42  7]\n",
      " [ 1  1 97 97]]\n",
      "Saved model by validation loss 1.0017399787902832\n",
      "\n",
      "Training accuracy and loss of epoch #17: 0.6419, 0.8570\n",
      "Validation after epoch #17, Validation Loss= 0.9935, Validation Accuracy= 0.515, Unweighted Accuracy= 0.573, Recall=  0.512\n",
      "[[53  6 20  4]\n",
      " [ 5 13 20  5]\n",
      " [10 11 41  5]\n",
      " [ 1  1 97 97]]\n",
      "Saved model by validation loss 0.9934984445571899\n",
      "Saved model by validation unweighted acc 0.5729204333160556\n",
      "\n",
      "Training accuracy and loss of epoch #18: 0.6460, 0.8464\n",
      "Validation after epoch #18, Validation Loss= 0.9877, Validation Accuracy= 0.512, Unweighted Accuracy= 0.565, Recall=  0.506\n",
      "[[53  7 20  4]\n",
      " [ 3 12 19  4]\n",
      " [12 11 41  6]\n",
      " [ 1  1 98 97]]\n",
      "Saved model by validation loss 0.9876939058303833\n",
      "\n",
      "Training accuracy and loss of epoch #19: 0.6499, 0.8356\n",
      "Validation after epoch #19, Validation Loss= 0.9797, Validation Accuracy= 0.515, Unweighted Accuracy= 0.565, Recall=  0.508\n",
      "[[53  7 19  4]\n",
      " [ 3 12 20  5]\n",
      " [12 11 43  6]\n",
      " [ 1  1 96 96]]\n",
      "Saved model by validation loss 0.97970050573349\n",
      "\n",
      "Training accuracy and loss of epoch #20: 0.6497, 0.8253\n",
      "Validation after epoch #20, Validation Loss= 0.9730, Validation Accuracy= 0.511, Unweighted Accuracy= 0.553, Recall=  0.497\n",
      "[[53  8 19  4]\n",
      " [ 3 11 19  6]\n",
      " [13 11 45  8]\n",
      " [ 0  1 95 93]]\n",
      "Saved model by validation loss 0.973007082939148\n",
      "\n",
      "Training accuracy and loss of epoch #21: 0.6559, 0.8158\n",
      "Validation after epoch #21, Validation Loss= 0.9695, Validation Accuracy= 0.513, Unweighted Accuracy= 0.557, Recall=  0.500\n",
      "[[54  8 19  4]\n",
      " [ 2 11 19  6]\n",
      " [13 11 45  8]\n",
      " [ 0  1 95 93]]\n",
      "Saved model by validation loss 0.9694715738296509\n",
      "\n",
      "Training accuracy and loss of epoch #22: 0.6589, 0.8065\n",
      "Validation after epoch #22, Validation Loss= 0.9633, Validation Accuracy= 0.513, Unweighted Accuracy= 0.559, Recall=  0.499\n",
      "[[55  8 20  4]\n",
      " [ 2 11 20  6]\n",
      " [12 11 44  8]\n",
      " [ 0  1 94 93]]\n",
      "Saved model by validation loss 0.9633392691612244\n",
      "\n",
      "Training accuracy and loss of epoch #23: 0.6602, 0.7977\n",
      "Validation after epoch #23, Validation Loss= 0.9599, Validation Accuracy= 0.519, Unweighted Accuracy= 0.562, Recall=  0.502\n",
      "[[55  8 19  4]\n",
      " [ 2 11 21  6]\n",
      " [12 12 46  8]\n",
      " [ 0  0 92 93]]\n",
      "Saved model by validation loss 0.9598580002784729\n",
      "\n",
      "Training accuracy and loss of epoch #24: 0.6624, 0.7893\n",
      "Validation after epoch #24, Validation Loss= 0.9548, Validation Accuracy= 0.519, Unweighted Accuracy= 0.562, Recall=  0.501\n",
      "[[55  8 20  4]\n",
      " [ 2 11 21  6]\n",
      " [12 12 46  8]\n",
      " [ 0  0 91 93]]\n",
      "Saved model by validation loss 0.9547885060310364\n",
      "\n",
      "Training accuracy and loss of epoch #25: 0.6657, 0.7814\n",
      "Validation after epoch #25, Validation Loss= 0.9517, Validation Accuracy= 0.519, Unweighted Accuracy= 0.562, Recall=  0.501\n",
      "[[55  8 19  4]\n",
      " [ 2 11 22  6]\n",
      " [12 12 46  8]\n",
      " [ 0  0 91 93]]\n",
      "Saved model by validation loss 0.9517042636871338\n",
      "\n",
      "Training accuracy and loss of epoch #26: 0.6657, 0.7736\n",
      "Validation after epoch #26, Validation Loss= 0.9446, Validation Accuracy= 0.514, Unweighted Accuracy= 0.559, Recall=  0.494\n",
      "[[55  8 21  4]\n",
      " [ 2 11 22  6]\n",
      " [12 12 44  8]\n",
      " [ 0  0 91 93]]\n",
      "Saved model by validation loss 0.9446133375167847\n",
      "\n",
      "Training accuracy and loss of epoch #27: 0.6692, 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after epoch #27, Validation Loss= 0.9433, Validation Accuracy= 0.519, Unweighted Accuracy= 0.555, Recall=  0.496\n",
      "[[55  8 20  4]\n",
      " [ 2 10 21  6]\n",
      " [12 13 47  8]\n",
      " [ 0  0 90 93]]\n",
      "Saved model by validation loss 0.9433112144470215\n",
      "\n",
      "Training accuracy and loss of epoch #28: 0.6739, 0.7585\n",
      "Validation after epoch #28, Validation Loss= 0.9389, Validation Accuracy= 0.519, Unweighted Accuracy= 0.555, Recall=  0.495\n",
      "[[55  9 21  4]\n",
      " [ 1 10 21  6]\n",
      " [13 12 47  8]\n",
      " [ 0  0 89 93]]\n",
      "Saved model by validation loss 0.9389028549194336\n",
      "\n",
      "Training accuracy and loss of epoch #29: 0.6789, 0.7507\n",
      "Validation after epoch #29, Validation Loss= 0.9372, Validation Accuracy= 0.519, Unweighted Accuracy= 0.555, Recall=  0.495\n",
      "[[55  9 20  3]\n",
      " [ 1 10 22  6]\n",
      " [13 12 47  9]\n",
      " [ 0  0 89 93]]\n",
      "Saved model by validation loss 0.9372427463531494\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "keep_rnn_prob = tf.placeholder(tf.float32)\n",
    "keep_fully_prob = tf.placeholder(tf.float32)\n",
    "normalization_switch = tf.placeholder(tf.bool)\n",
    "\n",
    "logits = nn_pipeline_rnn(X, weights_rnn, biases_rnn, keep_prob, keep_rnn_prob, keep_fully_prob, normalization_switch)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "for key in all_speakers:\n",
    "    train_data, train_labels, class_weights = prepare_train_data(all_speakers, key, dmean, dstd)\n",
    "    \n",
    "    weights = tf.reduce_sum(class_weights * Y, axis=1)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n",
    "    weighted_entropy = cross_entropy * weights\n",
    "    loss_function = tf.reduce_mean(weighted_entropy)\n",
    "    cross = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_trigger = optimizer.minimize(loss_function)\n",
    "\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for subkey in all_speakers[key]:\n",
    "        \n",
    "        test_data, test_labels, validation_data, validation_labels = prepare_validation_test_sets(all_speakers, subkey, key, dmean, dstd)\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "\n",
    "            print('Session initialized with session {0} and test speaker {1}.'.format(key, subkey))\n",
    "            train_network(sess, train_data, train_labels, validation_data, validation_labels, key, subkey)\n",
    "            \n",
    "            saver.restore(sess, checkpoint_folder + \"uacc_session{0}_{1}/best_model_uacc.ckpt\".format(key, subkey))    \n",
    "            \n",
    "            evaluate_model(sess, key, subkey)\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "FindFirstFile failed for: ./acrnn_topology1loss : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d80fd700066a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./{0}loss/best_model_loss.ckpt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marcelk\\pycharmprojects\\bcmarcel\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1526\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1528\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[0;32m   1530\u001b[0m                        + compat.as_text(save_path))\n",
      "\u001b[1;32mc:\\users\\marcelk\\pycharmprojects\\bcmarcel\\venv\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[1;34m(checkpoint_prefix)\u001b[0m\n\u001b[0;32m    362\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[0;32m    363\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[1;32m--> 364\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marcelk\\pycharmprojects\\bcmarcel\\venv\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    340\u001b[0m           \u001b[1;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m    344\u001b[0m               compat.as_bytes(single_filename), status)\n",
      "\u001b[1;32mc:\\users\\marcelk\\pycharmprojects\\bcmarcel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: FindFirstFile failed for: ./acrnn_topology1loss : The system cannot find the path specified.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    sess = tf.Session()\n",
    "\n",
    "    saver.restore(sess, \"./{0}loss/best_model_loss.ckpt\".format(checkpoint_folder))    \n",
    "    train_network(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "saver.restore(sess, \"./{0}uacc/best_model_uacc.ckpt\".format(checkpoint_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session1_M/best_model_loss.ckpt\n",
      "Acc: 0.7268292682926829, UAcc: 0.7435745060949979, Recall: 0.6742661120673169\n",
      "[[23  3  9  1]\n",
      " [ 0  8  8  0]\n",
      " [ 2  4 61  3]\n",
      " [ 0  0 26 57]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session1_F/best_model_loss.ckpt\n",
      "Acc: 0.4976958525345622, UAcc: 0.5743295327768652, Recall: 0.49911473760157976\n",
      "[[25  4  9  0]\n",
      " [ 1  7 15  1]\n",
      " [11  7 36  2]\n",
      " [ 0  0 59 40]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session2_M/best_model_loss.ckpt\n",
      "Acc: 0.6153846153846154, UAcc: 0.6686558402007763, Recall: 0.6209779019925857\n",
      "[[ 9  2  2  0]\n",
      " [ 2 19 43  0]\n",
      " [ 4  8 67  4]\n",
      " [ 0  0 15 33]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session2_F/best_model_loss.ckpt\n",
      "Acc: 0.5783783783783784, UAcc: 0.6368253968253968, Recall: 0.5306818181818183\n",
      "[[ 7  7 23  3]\n",
      " [ 0  9 11  8]\n",
      " [ 0  8 54 15]\n",
      " [ 0  1  2 37]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session3_M/best_model_loss.ckpt\n",
      "Acc: 0.6642335766423357, UAcc: 0.6916532596936478, Recall: 0.6448255176196352\n",
      "[[59  3  5  1]\n",
      " [ 7 14 39  0]\n",
      " [ 2  3 39 11]\n",
      " [ 0  1 20 70]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session3_F/best_model_loss.ckpt\n",
      "Acc: 0.5378787878787878, UAcc: 0.5702763123815755, Recall: 0.49239105953779866\n",
      "[[19 14 37 10]\n",
      " [ 2 16 31  1]\n",
      " [ 1  6 13  3]\n",
      " [ 0  3 14 94]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session4_M/best_model_loss.ckpt\n",
      "Acc: 0.6515837104072398, UAcc: 0.6806726151238347, Recall: 0.6497693366640629\n",
      "[[24  2 17  0]\n",
      " [ 4 15 30  1]\n",
      " [ 9  4 75  9]\n",
      " [ 0  0  1 30]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session4_F/best_model_loss.ckpt\n",
      "Acc: 0.5771812080536913, UAcc: 0.4765713239109863, Recall: 0.5012958532695375\n",
      "[[43  6 24  3]\n",
      " [ 2  0 14  0]\n",
      " [ 1  4 12  7]\n",
      " [ 1  0  1 31]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session5_M/best_model_loss.ckpt\n",
      "Acc: 0.5482625482625483, UAcc: 0.5160426093514329, Recall: 0.4739100768512533\n",
      "[[ 9  4  4  9]\n",
      " [ 0 12 14  2]\n",
      " [ 2 33 95 23]\n",
      " [ 2  1 23 26]]\n",
      "INFO:tensorflow:Restoring parameters from acrnn_topology2_es25loss_session5_F/best_model_loss.ckpt\n",
      "Acc: 0.5738255033557047, UAcc: 0.5779923183703914, Recall: 0.4890847593416947\n",
      "[[14 14 25  2]\n",
      " [ 2 10 13  5]\n",
      " [ 2 24 93 12]\n",
      " [ 0  8 20 54]]\n"
     ]
    }
   ],
   "source": [
    "test_accs = []\n",
    "test_uaccs = []\n",
    "test_recalls = []\n",
    "test_matrices = []\n",
    "\n",
    "for key in all_speakers:\n",
    "    for subkey in all_speakers[key]:\n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, checkpoint_folder + \"loss_session{0}_{1}/best_model_loss.ckpt\".format(key, subkey))        \n",
    "        tacc, tuacc, trec, tmat = evaluate_model_results(sess, key, subkey)\n",
    "        test_accs.append(tacc)\n",
    "        test_uaccs.append(tuacc)\n",
    "        test_recalls.append(trec)\n",
    "        test_matrices.append(tmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5971253449190548 0.6136593714729905 0.5576317173127283 0.584305528208686\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_accs), np.mean(test_uaccs), np.mean(test_recalls), (2*(np.mean(test_uaccs)*np.mean(test_recalls))/(np.mean(test_uaccs)+np.mean(test_recalls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[232,  59, 155,  29],\n",
       "       [ 20, 110, 218,  18],\n",
       "       [ 34, 101, 545,  89],\n",
       "       [  3,  14, 181, 472]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(op.add, test_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(session, data, labels):\n",
    "    predictions = session.run(tf.argmax(prediction, 1), feed_dict={X:data, Y:labels, keep_prob:1.0, keep_rnn_prob: 1.0, normalization_switch: False})\n",
    "    true_labels = [onehot.index(1.0) for onehot in labels]\n",
    "    return confusion_matrix(predictions, true_labels)\n",
    "\n",
    "def get_final_results(session, data, labels):\n",
    "    final_dict = {'ang': 0, 'hap': 0, 'neu': 0, 'sad': 0}\n",
    "    overall_dict = {'ang': 0, 'hap': 0, 'neu': 0, 'sad': 0}\n",
    "    predictions = session.run(correct_prediction, feed_dict={X:data, Y:labels, keep_prob:1.0,  keep_rnn_prob: 1.0,  normalization_switch: False})\n",
    "    coupled = list(zip(predictions, onehot_decode(labels)))\n",
    "    \n",
    "    for couple in coupled:\n",
    "        if (couple[0]):\n",
    "            final_dict[couple[1]] += 1\n",
    "        overall_dict[couple[1]] += 1\n",
    "    \n",
    "    return final_dict, overall_dict\n",
    "\n",
    "def get_final_results_glued(session, labels, correct):\n",
    "    final_dict = {'ang': 0, 'hap': 0, 'neu': 0, 'sad': 0}\n",
    "    overall_dict = {'ang': 0, 'hap': 0, 'neu': 0, 'sad': 0}\n",
    "    coupled = list(zip(correct, onehot_decode(labels)))\n",
    "    \n",
    "    for couple in coupled:\n",
    "        if (couple[0]):\n",
    "            final_dict[couple[1]] += 1\n",
    "        overall_dict[couple[1]] += 1\n",
    "    \n",
    "    return final_dict, overall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = np.asarray([])\n",
    "final_correct_preds = np.asarray([])\n",
    "final_probs = np.empty((0, 4))\n",
    "final_probs_softmax = np.empty((0, 4))\n",
    "\n",
    "\n",
    "midpoint = math.ceil(len(test_labels)/2)\n",
    "for i in range(0, 2):\n",
    "    predictions_test = sess.run(correct_prediction, feed_dict={X:test_data[i*midpoint:(i+1)*midpoint], Y:test_labels[i*midpoint:(i+1)*midpoint], keep_prob:1.0, keep_rnn_prob: 1.0, normalization_switch: False})\n",
    "    probabilities_test = sess.run(prediction, feed_dict={X:test_data[i*midpoint:(i+1)*midpoint], Y:test_labels[i*midpoint:(i+1)*midpoint], keep_prob:1.0, keep_rnn_prob: 1.0, normalization_switch: False})\n",
    "    predictions_test_confusion = sess.run(tf.argmax(prediction, 1), feed_dict={X:test_data[i*midpoint:(i+1)*midpoint], Y:test_labels[i*midpoint:(i+1)*midpoint], keep_prob:1.0, keep_rnn_prob: 1.0, normalization_switch: False})\n",
    "    predictions_test_softmax = sess.run(prediction, feed_dict={X:test_data[i*midpoint:(i+1)*midpoint], Y:test_labels[i*midpoint:(i+1)*midpoint], keep_prob:1.0, keep_rnn_prob: 1.0, normalization_switch: False})\n",
    "\n",
    "    final_correct_preds = np.concatenate((final_correct_preds, predictions_test))\n",
    "    final_probs = np.concatenate((final_probs, probabilities_test))\n",
    "    final_preds = np.concatenate((final_preds, predictions_test_confusion))\n",
    "    final_probs_softmax = np.concatenate((final_probs_softmax, predictions_test_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(final_preds, [onehot.index(1.0) for onehot in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(final_preds, [onehot.index(1.0) for onehot in test_labels])\n",
    "np.mean(np.diag(conf) / np.sum(conf, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict_test, overall_dict_test = get_final_results_glued(sess, test_labels, final_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dict_test)\n",
    "print(overall_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unweighted_accuracy(final_dict_test, overall_dict_test))\n",
    "print(weighted_accuracy(final_dict_test, overall_dict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(final_preds, [onehot.index(1.0) for onehot in test_labels], average='macro')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_probs_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = list(map(lambda line: float(line.split(\", \")[2]), open(checkpoint_folder + \"val_loss_logs.txt\")))\n",
    "val_accs = list(map(lambda line: float(line.split(\", \")[1]), open(checkpoint_folder + \"val_loss_logs.txt\")))\n",
    "val_uaccs = list(map(lambda line: float(line.split(\", \")[3]), open(checkpoint_folder + \"val_loss_logs.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = list(map(lambda line: float(line.split(\", \")[2]), open(checkpoint_folder + \"train_loss_logs.txt\")))\n",
    "train_accs = list(map(lambda line: float(line.split(\", \")[1]), open(checkpoint_folder + \"train_loss_logs.txt\")))\n",
    "epochs = list(map(lambda line: int(line.split(\", \")[0]), open(checkpoint_folder + \"train_loss_logs.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs, train_losses, epochs, val_losses)\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.savefig(\"./graphs_validation/{0}loss.png\".format(checkpoint_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(epochs, train_accs, epochs, val_accs)\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.savefig(\"./graphs_validation/{0}acc.png\".format(checkpoint_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(epochs, val_uaccs)\n",
    "plt.legend(['Validation'])\n",
    "plt.savefig(\"./graphs_validation/{0}uacc.png\".format(checkpoint_folder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
