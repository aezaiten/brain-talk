{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from functools import reduce\n",
    "import operator as op\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dataset_folder = os.path.abspath(\"./individual_npzs/{0}/*.npz\")\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "dropout = 0.75\n",
    "max_pool = 2\n",
    "strides = 1\n",
    "input_size = 60000\n",
    "output_size = 4\n",
    "epochs = 30\n",
    "timesteps = 38\n",
    "seed_num = 72\n",
    "hidden_layer = 64\n",
    "l2_beta = 0.01\n",
    "\n",
    "validation_session = 2\n",
    "test_session = 1\n",
    "\n",
    "label_dictionary = {'ang': 0, 'hap': 1, 'neu': 2, 'sad': 3}\n",
    "onehot_dictionary = {0: 'ang', 1: 'hap', 2:'neu', 3:'sad'}\n",
    "\n",
    "tf.set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, labels):\n",
    "    steps = math.ceil(data.shape[0] / batch_size)\n",
    "    for batch_step in range(0, steps):\n",
    "        start = batch_size * batch_step\n",
    "        end = batch_size * (batch_step + 1)\n",
    "        yield data[start:end], labels[start:end], batch_step\n",
    "        \n",
    "def build_encoded_array(emotion_label):\n",
    "    initialized_array = [0. for key in label_dictionary]\n",
    "    initialized_array[label_dictionary[emotion_label]] = 1.\n",
    "    return initialized_array\n",
    "        \n",
    "def onehot_encode(label_minibatch):\n",
    "    return [build_encoded_array(emotion_label) for emotion_label in label_minibatch]\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    return np.ndarray.tolist(class_weight.compute_class_weight('balanced', np.unique(labels), labels))\n",
    "\n",
    "def sparse_encode(label_minibatch):\n",
    "    return [label_dictionary[emotion_label] for emotion_label in label_minibatch]\n",
    "\n",
    "def onehot_decode(onehot_labels):\n",
    "    return [onehot_dictionary[label.index(1.0)] for label in onehot_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset: 3 sessions for training, 1 for validation, 1 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "validation_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "train_labels = []\n",
    "validation_labels = []\n",
    "test_labels = []\n",
    "\n",
    "all_sessions = {1: [], 2:[], 3:[], 4:[], 5:[]}\n",
    "\n",
    "session_string = 'session{0}'\n",
    "\n",
    "for i in range(1, 6):\n",
    "    formatted = session_string.format(i)\n",
    "    for spectrogram in glob.glob(dataset_folder.format(formatted)):\n",
    "        loaded_spec = np.load(spectrogram)\n",
    "        for x in loaded_spec['spectrograms']:\n",
    "            if i != validation_session and i != test_session:\n",
    "                train_dataset.append(x) \n",
    "            elif i == validation_session:\n",
    "                validation_dataset.append(x)\n",
    "            elif i == test_session:\n",
    "                test_dataset.append(x)\n",
    "        for x in loaded_spec['labels']:\n",
    "            all_sessions[i].append(x)\n",
    "            if i != validation_session and i != test_session:\n",
    "                train_labels.append(x) \n",
    "            elif i == validation_session:\n",
    "                validation_labels.append(x)\n",
    "            elif i == test_session:\n",
    "                test_labels.append(x)\n",
    "        \n",
    "train_dataset = np.asarray(train_dataset)\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "validation_dataset = np.asarray(validation_dataset)\n",
    "validation_labels = np.asarray(validation_labels)\n",
    "\n",
    "test_dataset = np.asarray(test_dataset)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session #1 (array(['ang', 'hap', 'neu', 'sad'], dtype='<U3'), array([ 84,  46, 275, 173], dtype=int64))\n",
      "Session #2 (array(['ang', 'hap', 'neu', 'sad'], dtype='<U3'), array([ 48,  81, 422, 193], dtype=int64))\n",
      "Session #3 (array(['ang', 'hap', 'neu', 'sad'], dtype='<U3'), array([124,  82, 282, 258], dtype=int64))\n",
      "Session #4 (array(['ang', 'hap', 'neu', 'sad'], dtype='<U3'), array([140,  60, 248, 175], dtype=int64))\n",
      "Session #5 (array(['ang', 'hap', 'neu', 'sad'], dtype='<U3'), array([ 56, 157, 432, 221], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(\"Session #{0}\".format(session), np.unique(all_sessions[session], return_counts=True)) for session in all_sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.zeros([len(train_dataset), train_dataset[0].shape[0], train_dataset[0].shape[1]], dtype=np.uint8)\n",
    "for data in range(len(train_dataset)):\n",
    "    train_data[data,:,:] = train_dataset[data]\n",
    "    \n",
    "validation_data = np.zeros([len(validation_dataset), validation_dataset[0].shape[0], validation_dataset[0].shape[1]], dtype=np.uint8)\n",
    "for data in range(len(validation_dataset)):\n",
    "    validation_data[data,:,:] = validation_dataset[data]\n",
    "    \n",
    "test_data = np.zeros([len(test_dataset), test_dataset[0].shape[0], test_dataset[0].shape[1]], dtype=np.uint8)\n",
    "for data in range(len(test_dataset)):\n",
    "    test_data[data,:,:] = test_dataset[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#class_weights = compute_class_weights(train_labels)\n",
    "counts = np.unique(train_labels, return_counts=True)[1]\n",
    "sum_counts = sum(counts)\n",
    "class_weights = [count/sum_counts for count in counts]\n",
    "#class_weights = list(map(lambda x: x if x > 1.0 else 1.0, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14317673378076062,\n",
       " 0.1337807606263982,\n",
       " 0.4304250559284116,\n",
       " 0.29261744966442954]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.reshape((validation_data.shape[0], input_size))\n",
    "validation_labels = onehot_encode(validation_labels)\n",
    "\n",
    "test_data = test_data.reshape((test_data.shape[0], input_size))\n",
    "test_labels = onehot_encode(test_labels)\n",
    "\n",
    "train_data = train_data.reshape((train_data.shape[0], input_size))\n",
    "train_labels = onehot_encode(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=seed_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(to_process, weights, biases, strides=1):\n",
    "    conv_out = tf.nn.conv2d(to_process, weights, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    bias_out = tf.nn.bias_add(conv_out, biases)\n",
    "    relu_out = tf.nn.relu(bias_out)\n",
    "    return relu_out\n",
    "\n",
    "def maxpool2d(to_pool, pool_size=2):\n",
    "    maxpool_out = tf.nn.max_pool(to_pool, ksize=[1, pool_size, pool_size, 1], strides=[1, pool_size, pool_size, 1], padding='SAME')\n",
    "    return maxpool_out\n",
    "\n",
    "def nn_pipeline(spectrogram, weights, biases):\n",
    "    \n",
    "    reshaped_input = tf.reshape(spectrogram, shape=[-1, 200, 300, 1])\n",
    "    \n",
    "    first_layer_out = conv2d(reshaped_input, weights['first_layer_weights'], biases['first_layer_biases'])\n",
    "    first_maxpool_out = maxpool2d(first_layer_out, pool_size=2)\n",
    "    \n",
    "    second_layer_out = conv2d(first_maxpool_out, weights['second_layer_weights'], biases['second_layer_biases'])\n",
    "    second_maxpool_out = maxpool2d(second_layer_out, pool_size=2)\n",
    "    \n",
    "    third_layer_out = conv2d(second_maxpool_out, weights['third_layer_weights'], biases['third_layer_biases'])\n",
    "    third_maxpool_out = maxpool2d(third_layer_out, pool_size=2)\n",
    "    \n",
    "    reshape_for_fc = tf.reshape(third_maxpool_out, [-1, weights['fully_connected_weights'].get_shape().as_list()[0]])\n",
    "    fully_connected_out = tf.add(tf.matmul(reshape_for_fc, weights['fully_connected_weights']), biases['fully_connected_biases'])\n",
    "    fully_connected_activation = tf.nn.relu(fully_connected_out)\n",
    "    fully_connected_dropout = tf.nn.dropout(fully_connected_activation, dropout)\n",
    "    \n",
    "    fully_connected_out_2 = tf.add(tf.matmul(fully_connected_dropout, weights['fully_connected_weights_2']), biases['fully_connected_biases_2'])\n",
    "    fully_connected_activation_2 = tf.nn.relu(fully_connected_out_2)\n",
    "    fully_connected_dropout_2 = tf.nn.dropout(fully_connected_activation_2, dropout)\n",
    "    \n",
    "    prediction = tf.add(tf.matmul(fully_connected_dropout_2, weights['output']), biases['output'])\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_pipeline_rnn(spectrogram, weights, biases, dropout_num, normalization_switch):\n",
    "    reshaped_input = tf.reshape(spectrogram, shape=[-1, 200, 300, 1])\n",
    "\n",
    "    first_layer_out = conv2d(reshaped_input, weights['first_layer_weights'], biases['first_layer_biases'])\n",
    "    first_maxpool_out = maxpool2d(first_layer_out, pool_size=2)\n",
    "    first_batch_norm = tf.layers.batch_normalization(first_maxpool_out, training=normalization_switch)\n",
    "\n",
    "    second_layer_out = conv2d(first_batch_norm, weights['second_layer_weights'], biases['second_layer_biases'])\n",
    "    second_maxpool_out = maxpool2d(second_layer_out, pool_size=2)\n",
    "    second_batch_norm = tf.layers.batch_normalization(second_maxpool_out, training=normalization_switch)\n",
    "\n",
    "    third_layer_out = conv2d(second_batch_norm, weights['third_layer_weights'], biases['third_layer_biases'])\n",
    "    third_maxpool_out = maxpool2d(third_layer_out, pool_size=2)\n",
    "    third_batch_norm = tf.layers.batch_normalization(third_maxpool_out, training=normalization_switch)\n",
    "\n",
    "    interim_shape = third_batch_norm.get_shape().as_list()\n",
    "    transposed = tf.transpose(third_maxpool_out, perm=[0, 2, 1, 3])\n",
    "    reshape_for_rnn = tf.reshape(transposed, [-1, interim_shape[2], interim_shape[1]*interim_shape[3]])\n",
    "    reshape_for_rnn.set_shape([None, interim_shape[2], interim_shape[1]*interim_shape[3]])\n",
    "\n",
    "    hidden_list = [hidden_layer, hidden_layer]\n",
    "\n",
    "    gru_fw_cell = [tf.contrib.rnn.GRUCell(hidden) for hidden in hidden_list]\n",
    "    gru_bw_cell = [tf.contrib.rnn.GRUCell(hidden) for hidden in hidden_list]\n",
    "\n",
    "    gru_output, _, _, = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(gru_fw_cell, gru_bw_cell, reshape_for_rnn, dtype=tf.float32)\n",
    "    interim_shape_gru = tf.shape(gru_output)\n",
    "    gru_flatten = tf.reshape(gru_output, [-1, interim_shape_gru[1]*interim_shape_gru[2]])\n",
    "    \n",
    "    fully_connected_out = tf.add(tf.matmul(gru_flatten, weights['gru_weights']), biases['gru_biases'])\n",
    "    fully_connected_activation = tf.nn.relu(fully_connected_out)\n",
    "    fully_connected_dropout = tf.nn.dropout(fully_connected_activation, dropout_num)\n",
    "    \n",
    "    prediction = tf.add(tf.matmul(fully_connected_dropout, weights['output']), biases['output'])\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'first_layer_weights': tf.Variable(tf.random_normal([10, 15, 1, 16])),\n",
    "    'second_layer_weights': tf.Variable(tf.random_normal([8, 10, 16, 24])),\n",
    "    'third_layer_weights': tf.Variable(tf.random_normal([5, 8, 24, 32])),\n",
    "    'fully_connected_weights': tf.Variable(tf.random_normal([25*38*32, 2048])),\n",
    "    'fully_connected_weights_2': tf.Variable(tf.random_normal([2048, 2048])),\n",
    "    'output': tf.Variable(tf.random_normal([2048, output_size]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'first_layer_biases': tf.Variable(tf.random_normal([16])),\n",
    "    'second_layer_biases': tf.Variable(tf.random_normal([24])),\n",
    "    'third_layer_biases': tf.Variable(tf.random_normal([32])),\n",
    "    'fully_connected_biases': tf.Variable(tf.random_normal([2048])),\n",
    "    'fully_connected_biases_2': tf.Variable(tf.random_normal([2048])),\n",
    "    'output': tf.Variable(tf.random_normal([output_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_rnn = {\n",
    "    'first_layer_weights': tf.Variable(tf.truncated_normal([10, 15, 1, 16], seed=seed_num)),\n",
    "    'second_layer_weights': tf.Variable(tf.truncated_normal([8, 10, 16, 24], seed=seed_num)),\n",
    "    'third_layer_weights': tf.Variable(tf.truncated_normal([5, 8, 24, 32], seed=seed_num)),\n",
    "    'gru_weights': tf.Variable(tf.truncated_normal([2*hidden_layer*timesteps, hidden_layer], seed=seed_num)),\n",
    "    'output': tf.Variable(tf.truncated_normal([hidden_layer, output_size], seed=seed_num))\n",
    "}\n",
    "\n",
    "biases_rnn = {\n",
    "    'first_layer_biases': tf.Variable(tf.truncated_normal([16], seed=seed_num)),\n",
    "    'second_layer_biases': tf.Variable(tf.truncated_normal([24], seed=seed_num)),\n",
    "    'third_layer_biases': tf.Variable(tf.truncated_normal([32], seed=seed_num)),\n",
    "    'gru_biases': tf.Variable(tf.truncated_normal([hidden_layer], seed=seed_num)),\n",
    "    'output': tf.Variable(tf.truncated_normal([output_size], seed=seed_num))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "normalization_switch = tf.placeholder(tf.bool)\n",
    "\n",
    "logits = nn_pipeline_rnn(X, weights_rnn, biases_rnn, keep_prob, normalization_switch)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "weighted_logits = tf.multiply(class_weights, logits)\n",
    "#weight_regularizer = tf.add_n([tf.nn.l2_loss(weights_rnn[weights]) for weights in weights_rnn]) * 0.01\n",
    "\n",
    "loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=weighted_logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_trigger = optimizer.minimize(loss_function)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session initialized.\n",
      "Validation after epoch #1, Validation Loss= 9.9872, Validation Accuracy= 0.442\n",
      "Validation after epoch #2, Validation Loss= 3.9379, Validation Accuracy= 0.335\n",
      "Validation after epoch #3, Validation Loss= 2.5752, Validation Accuracy= 0.309\n",
      "Validation after epoch #4, Validation Loss= 2.0761, Validation Accuracy= 0.304\n",
      "Validation after epoch #5, Validation Loss= 1.8333, Validation Accuracy= 0.296\n",
      "[1 1 1 0 0 2 0 3 0 2 0 1 0 3 3 1 1 0 2 0 3 2 2 2 0 2 2 0 3 0 3 0 0 3 3 0 3\n",
      " 2 0 1 1 0 0 0 2 1 3 2 3 1 1 2 3 3 3 3 3 0 1 1 0 2 0 2 0 0 2 0 1 0 2 3 0 0\n",
      " 1 2 3 0 3 1 0 0 0 1 1 2 1 3 1 1 0 0 2 3 3 1 0 2 0 2 2 1 0 2 1 0 0 2 2 1 3\n",
      " 2 0 3 0 2 1 1 3 1 3 0 3 2 1 3 0 3 2 1 2 2 0 2 3 1 1 0 2 2 0 0 1 0 0 0 1 0\n",
      " 3 0 0 0 0 2 1 2 0 0 0 2 2 0 2 2 2 2 0 0 3 1 3 1 3 2 0 2 1 3 0 2 3 3 1 0 0\n",
      " 2 1 0 0 0 0 3 0 0 1 0 1 2 3 2 3 0 0 0 0 3 1 2 2 3 2 2 1 1 0 1 1 2 3 0 2 0\n",
      " 2 3 2 2 0 2 1 3 3 0 1 1 3 0 1 0 3 3 1 2 1 2 2 1 3 1 3 0 3 2 1 0 3 3 2 3 2\n",
      " 1 2 2 2 1 3 1 3 1 2 2 0 1 3 3 2 1 0 3 0 2 3 0 0 0 1 3 2 3 0 2 3 2 3 2 1 1\n",
      " 3 2 1 2 0 1 0 0 0 1 0 1 1 0 0 0 2 1 2 1 0 2 0 0 2 2 0 0 0 0 0 3 1 2 0 0 0\n",
      " 3 0 2 3 2 0 0 0 0 0 0 2 0 1 0 0 2 0 1 1 1 0 1 0 0 2 2 3 1 1 1 2 2 0 1 2 2\n",
      " 0 1 2 2 2 3 1 1 2 0 0 0 1 0 3 2 3 2 0 2 1 0 2 2 0 1 1 0 1 2 1 3 1 3 0 2 0\n",
      " 0 3 1 1 1 1 1 1 1 3 2 2 2 1 3 2 0 2 3 1 3 2 3 1 2 3 2 3 0 2 0 0 2 1 3 3 1\n",
      " 3 1 0 3 2 3 1 2 3 1 1 1 1 2 3 0 1 3 0 1 1 1 0 2 1 3 0 2 0 3 1 2 1 1 1 2 2\n",
      " 1 0 3 0 0 2 1 2 3 1 1 3 1 1 3 1 3 1 0 0 0 0 2 0 2 1 2 0 2 1 0 3 2 0 0 1 1\n",
      " 3 0 2 1 2 1 0 0 2 3 2 1 0 3 2 0 2 0 3 0 2 0 2 0 2 0 0 0 0 2 2 0 3 0 1 1 2\n",
      " 0 0 1 2 2 3 2 3 1 1 2 3 3 1 1 3 3 1 1 3 0 3 1 3 3 3 0 2 0 3 2 3 0 1 1 1 3\n",
      " 3 1 1 3 1 3 0 3 0 3 1 1 2 3 3 3 0 3 3 3 3 2 3 2 1 0 1 1 2 2 2 3 2 2 1 0 3\n",
      " 2 3 0 2 2 2 2 0 3 3 2 3 3 3 0 2 3 0 0 0 2 0 1 3 2 1 1 0 1 1 3 3 1 1 0 1 2\n",
      " 0 0 2 3 1 3 1 2 2 0 0 2 2 1 2 0 2 2 1 1 1 0 2 1 2 2 2 2 3 0 0 0 2 1 2 3 1\n",
      " 1 0 2 3 3 0 0 1 0 3 1 2 1 2 2 3 1 3 1 2 1 0 0 0 0 2 0 1 1 3 2 2 3 2 1 2 1\n",
      " 3 1 3 3]\n",
      "Validation after epoch #6, Validation Loss= 1.7156, Validation Accuracy= 0.300\n",
      "Validation after epoch #7, Validation Loss= 1.6462, Validation Accuracy= 0.348\n",
      "Validation after epoch #8, Validation Loss= 1.6183, Validation Accuracy= 0.336\n",
      "Validation after epoch #9, Validation Loss= 1.6095, Validation Accuracy= 0.351\n",
      "Validation after epoch #10, Validation Loss= 1.5758, Validation Accuracy= 0.337\n",
      "[1 1 1 1 0 2 0 1 0 2 0 2 2 3 3 1 2 0 2 3 3 2 2 2 0 3 3 0 0 2 3 1 3 3 3 3 3\n",
      " 3 3 2 1 0 3 0 2 3 3 2 3 1 3 0 3 3 0 3 3 2 3 2 2 0 2 0 0 0 0 0 1 2 2 3 1 0\n",
      " 1 2 2 0 3 1 1 0 3 0 1 3 1 3 2 1 0 0 2 1 3 1 2 2 0 2 3 3 0 2 3 3 3 2 3 2 2\n",
      " 2 0 3 0 2 3 3 2 1 3 0 3 2 3 3 2 2 2 1 2 3 2 2 3 1 1 0 2 2 0 0 1 0 0 0 1 3\n",
      " 3 0 2 2 0 2 1 2 0 3 3 2 2 2 2 2 2 2 3 0 3 1 3 1 3 2 0 3 1 3 0 3 0 3 1 3 0\n",
      " 2 3 0 1 0 1 3 0 3 1 0 1 2 3 3 3 0 0 0 0 3 1 3 3 3 3 3 1 2 0 1 1 3 3 0 2 0\n",
      " 3 3 2 0 0 2 1 3 3 1 3 3 3 2 3 0 3 3 1 2 1 2 2 1 3 3 3 2 3 2 1 3 3 3 3 3 2\n",
      " 1 2 2 2 2 3 3 3 1 3 3 0 3 3 3 3 2 0 3 3 2 3 0 3 0 1 3 2 3 0 0 3 2 3 2 1 2\n",
      " 3 2 1 2 2 2 0 0 2 1 0 1 1 0 0 1 1 1 2 3 2 3 0 0 2 2 0 0 0 0 2 3 1 2 0 1 0\n",
      " 3 0 0 3 2 0 0 0 0 0 0 2 0 1 0 0 2 2 1 2 1 0 1 0 0 2 2 0 2 3 3 2 1 3 1 3 2\n",
      " 0 3 2 3 2 1 0 0 2 0 0 0 1 2 3 3 3 2 3 2 1 0 2 0 1 2 2 0 1 2 1 3 1 2 0 2 2\n",
      " 0 3 1 2 3 1 1 1 1 3 3 2 2 3 3 2 2 2 3 1 3 2 3 3 2 3 2 3 0 0 0 3 1 1 3 3 2\n",
      " 3 2 0 3 2 3 1 2 3 1 1 1 1 2 3 2 1 3 0 1 2 1 0 2 1 3 0 2 0 2 1 2 1 1 3 2 3\n",
      " 0 0 0 0 0 1 1 2 2 0 2 3 1 3 3 2 3 0 0 0 0 0 3 0 2 1 3 2 3 1 0 3 2 0 3 2 1\n",
      " 3 0 2 1 2 2 0 0 2 2 0 1 0 2 2 0 1 2 3 2 2 2 2 2 3 0 0 0 2 2 2 2 2 0 1 1 2\n",
      " 2 2 3 2 2 3 2 3 1 1 1 3 3 3 3 3 3 3 3 3 2 3 1 3 3 3 2 3 0 3 2 3 2 1 1 2 2\n",
      " 3 3 1 3 3 3 0 3 0 3 2 1 3 3 3 3 0 3 3 3 3 3 3 2 1 2 1 1 2 3 2 2 2 2 1 3 3\n",
      " 2 3 3 2 2 2 2 2 3 3 2 3 3 3 0 3 3 0 2 2 2 2 1 3 0 1 1 0 1 2 3 2 1 1 0 1 2\n",
      " 0 0 3 3 1 3 2 2 2 0 0 2 2 1 2 0 3 2 1 1 2 0 3 3 2 3 1 3 3 0 0 0 2 2 3 2 1\n",
      " 1 0 2 3 1 3 1 3 1 3 1 2 1 2 1 3 1 3 1 2 1 0 0 3 2 2 0 1 3 3 2 2 3 3 1 2 3\n",
      " 3 1 3 2]\n",
      "Validation after epoch #11, Validation Loss= 1.5934, Validation Accuracy= 0.359\n",
      "Validation after epoch #12, Validation Loss= 1.5690, Validation Accuracy= 0.360\n",
      "Validation after epoch #13, Validation Loss= 1.5791, Validation Accuracy= 0.362\n",
      "Validation after epoch #14, Validation Loss= 1.5829, Validation Accuracy= 0.362\n",
      "Validation after epoch #15, Validation Loss= 1.5916, Validation Accuracy= 0.348\n",
      "[1 1 1 2 0 2 0 2 0 2 0 2 0 3 3 1 2 0 2 2 3 2 2 2 0 2 0 0 0 2 3 1 3 3 3 3 3\n",
      " 2 3 2 1 2 0 0 2 3 2 2 2 2 2 2 3 3 0 3 3 2 3 2 2 0 2 0 0 3 0 0 1 0 2 2 1 0\n",
      " 1 2 2 0 3 1 0 0 3 0 3 3 1 3 2 1 0 0 2 2 3 1 2 2 0 2 2 3 2 2 2 0 2 2 3 2 3\n",
      " 2 0 3 0 2 3 3 2 1 3 0 3 2 2 1 2 2 2 1 2 3 2 2 3 1 1 0 2 2 1 0 1 2 0 0 2 3\n",
      " 3 0 2 2 0 2 1 2 0 3 2 2 2 0 2 2 2 2 0 0 3 1 3 1 3 2 0 3 2 3 0 2 0 3 1 2 0\n",
      " 2 3 0 0 0 0 3 0 3 1 0 2 2 2 3 3 0 0 0 0 3 1 2 3 3 3 3 1 2 3 1 1 1 3 0 2 0\n",
      " 3 3 2 0 0 2 1 3 3 0 3 3 2 0 3 0 3 3 1 2 1 2 2 1 3 3 3 2 3 2 3 3 3 3 3 3 2\n",
      " 3 2 2 1 2 3 3 3 3 3 2 2 3 3 3 2 2 0 3 3 2 3 3 0 0 1 3 2 2 0 1 3 2 3 2 2 2\n",
      " 3 2 1 2 2 2 0 0 2 1 0 0 2 0 0 0 1 1 2 2 2 2 0 0 2 2 0 0 0 0 0 2 3 2 0 1 0\n",
      " 0 0 2 3 2 0 0 0 0 0 0 2 0 1 0 0 2 2 2 2 1 0 1 0 2 2 2 3 1 1 0 2 2 3 3 2 2\n",
      " 0 3 3 2 2 1 1 0 2 0 0 0 2 0 3 3 3 2 3 2 1 0 2 0 2 2 2 0 1 2 1 3 0 2 0 2 2\n",
      " 2 3 1 2 2 1 1 1 0 3 3 2 2 3 3 2 2 2 3 1 3 2 3 3 2 2 2 3 0 2 0 3 1 1 3 3 2\n",
      " 3 2 2 3 2 3 2 2 3 2 1 1 1 2 3 2 1 3 0 2 2 2 0 2 1 2 0 2 0 2 1 2 3 1 3 2 2\n",
      " 0 0 0 0 2 3 1 2 2 0 2 3 1 3 3 2 3 0 0 0 3 0 3 0 2 1 3 2 3 1 0 3 2 0 0 2 1\n",
      " 3 0 2 1 2 2 0 0 2 2 0 1 0 2 2 2 2 2 3 0 2 0 1 0 3 0 2 0 0 2 2 2 2 0 3 1 2\n",
      " 0 2 3 2 2 3 2 3 1 1 2 3 3 3 1 3 3 2 3 3 2 3 1 3 3 3 1 3 0 3 2 3 2 1 1 2 2\n",
      " 3 3 1 3 2 3 2 3 0 3 2 1 3 3 3 3 0 3 3 3 3 1 3 2 1 2 1 1 2 3 3 2 2 2 2 2 3\n",
      " 2 3 3 2 0 2 2 3 3 3 2 2 3 3 0 2 3 0 2 2 0 2 1 3 0 1 2 2 1 2 3 3 2 1 0 1 2\n",
      " 0 0 2 3 1 1 2 2 2 0 0 2 2 1 2 0 3 2 2 2 2 0 3 3 2 2 1 2 3 0 0 0 2 2 3 2 0\n",
      " 1 0 2 3 1 0 2 3 1 3 2 0 1 0 1 3 1 3 2 2 1 0 2 3 3 2 0 1 3 3 2 2 3 2 1 2 3\n",
      " 3 2 3 3]\n",
      "Validation after epoch #16, Validation Loss= 1.5891, Validation Accuracy= 0.356\n",
      "Validation after epoch #17, Validation Loss= 1.5955, Validation Accuracy= 0.368\n",
      "Validation after epoch #18, Validation Loss= 1.6183, Validation Accuracy= 0.366\n",
      "Validation after epoch #19, Validation Loss= 1.5963, Validation Accuracy= 0.363\n",
      "Validation after epoch #20, Validation Loss= 1.5908, Validation Accuracy= 0.370\n",
      "[1 1 1 1 0 2 0 2 0 2 0 2 2 3 3 1 2 0 2 2 3 2 2 2 0 2 0 0 0 2 3 1 3 3 3 3 3\n",
      " 2 3 3 1 2 0 0 2 3 3 2 3 2 2 0 3 3 0 3 3 2 3 2 0 0 0 0 0 3 0 0 1 0 2 3 1 0\n",
      " 1 2 2 0 3 1 0 0 3 0 1 3 1 3 1 1 0 0 2 2 3 1 2 2 0 3 2 3 2 2 1 0 2 2 3 2 3\n",
      " 2 0 3 0 2 2 3 2 1 3 0 3 2 2 1 2 2 2 1 2 2 2 2 3 1 1 0 1 2 1 0 1 2 0 0 2 3\n",
      " 3 0 2 0 0 2 1 2 0 2 2 2 2 0 2 2 2 2 0 0 3 1 3 1 3 2 0 3 2 3 0 3 0 3 1 2 0\n",
      " 2 3 0 0 0 0 2 0 3 1 3 2 2 2 3 3 0 0 0 0 3 1 2 3 3 3 3 1 2 3 1 2 1 3 2 2 0\n",
      " 3 3 2 2 0 2 1 3 3 0 3 3 3 0 3 0 3 3 1 2 1 2 2 1 3 3 3 2 3 2 3 3 3 3 3 3 3\n",
      " 3 2 2 2 2 3 1 3 3 3 2 2 3 3 3 2 2 0 3 3 2 3 3 0 0 1 3 2 2 0 0 3 2 3 2 2 2\n",
      " 3 2 1 2 2 2 0 0 2 1 0 0 2 0 2 1 1 1 2 2 2 3 0 0 2 2 0 0 0 0 2 3 3 2 0 1 0\n",
      " 0 0 2 3 2 0 0 0 0 0 0 2 0 1 0 0 2 2 1 2 1 0 1 0 0 2 2 0 1 1 0 2 2 3 3 2 2\n",
      " 0 2 3 2 2 1 1 0 0 0 0 0 2 2 3 3 3 2 3 2 1 0 2 2 2 2 2 0 1 2 1 3 0 2 0 2 2\n",
      " 2 3 1 2 2 1 1 3 1 3 3 2 2 3 2 3 2 2 3 1 3 2 3 3 2 2 2 3 0 2 0 3 1 1 3 2 2\n",
      " 3 2 2 3 2 3 2 2 3 2 1 1 1 2 3 2 1 3 0 3 2 2 0 2 1 2 0 2 0 2 1 2 3 1 3 2 2\n",
      " 0 0 3 0 2 1 2 2 2 0 2 3 1 3 3 2 3 0 0 0 0 2 3 0 2 1 3 0 3 1 0 3 2 0 2 2 1\n",
      " 3 0 2 1 2 2 0 0 2 2 2 1 0 2 2 2 2 2 3 0 2 0 1 2 3 0 2 0 0 2 2 2 2 0 3 1 2\n",
      " 0 2 3 2 2 3 2 3 1 3 2 3 3 3 2 3 3 2 3 3 2 3 1 3 3 3 2 3 0 3 2 3 2 1 1 2 2\n",
      " 3 3 1 3 2 3 0 3 2 3 2 1 3 3 3 3 0 3 3 3 3 2 3 2 1 2 3 1 2 3 3 3 2 2 1 3 3\n",
      " 2 1 2 2 2 2 2 3 3 3 2 3 3 3 0 2 3 0 2 2 2 2 3 3 0 1 2 2 1 2 3 3 2 1 0 1 2\n",
      " 0 0 2 3 1 0 2 2 2 0 0 2 2 1 2 0 3 2 2 2 2 0 3 3 2 2 1 2 3 0 0 0 2 2 3 2 0\n",
      " 1 0 2 3 1 0 2 3 1 3 2 2 1 2 1 3 1 3 3 2 2 0 2 3 3 2 0 1 3 3 2 2 3 2 1 2 3\n",
      " 3 2 3 2]\n",
      "Validation after epoch #21, Validation Loss= 1.5954, Validation Accuracy= 0.360\n",
      "Validation after epoch #22, Validation Loss= 1.6049, Validation Accuracy= 0.376\n",
      "Validation after epoch #23, Validation Loss= 1.6023, Validation Accuracy= 0.364\n",
      "Validation after epoch #24, Validation Loss= 1.5967, Validation Accuracy= 0.364\n",
      "Validation after epoch #25, Validation Loss= 1.6220, Validation Accuracy= 0.359\n",
      "[2 1 1 2 0 2 0 3 0 2 2 2 0 3 3 1 2 0 0 2 3 2 2 2 0 2 2 0 0 2 3 1 3 3 3 3 3\n",
      " 2 0 3 1 2 0 0 2 3 2 2 2 2 3 2 3 3 0 3 3 2 3 2 2 0 0 0 0 3 0 0 1 0 2 3 1 0\n",
      " 2 2 3 0 3 1 0 0 3 2 3 3 1 3 0 1 0 0 2 2 3 1 2 2 0 2 2 3 2 2 3 2 2 2 3 0 3\n",
      " 2 0 3 0 2 3 3 2 1 3 0 3 2 3 1 2 2 2 1 2 2 0 2 3 1 1 0 1 2 0 0 1 2 0 0 1 3\n",
      " 3 0 2 2 0 2 1 2 0 3 2 2 2 0 2 2 2 2 0 0 3 1 3 1 3 2 0 3 2 3 0 3 0 3 1 0 0\n",
      " 3 3 0 0 2 0 3 0 3 1 0 2 2 3 3 3 0 0 0 0 3 1 2 3 3 3 3 1 2 3 1 2 1 3 2 2 0\n",
      " 3 3 2 2 0 3 1 3 3 0 3 3 3 0 3 0 3 3 1 2 1 2 2 1 3 3 3 2 3 3 1 3 3 3 3 3 2\n",
      " 1 2 2 2 2 3 3 3 1 3 2 2 3 3 3 3 2 0 3 3 2 3 3 0 0 1 3 2 2 0 2 3 2 3 2 2 2\n",
      " 3 2 1 2 2 2 0 0 2 0 0 0 2 0 2 0 1 1 2 2 2 3 0 0 2 2 0 0 0 0 0 2 3 2 0 1 1\n",
      " 0 0 0 3 2 0 0 0 0 0 0 2 0 1 0 0 2 2 2 2 1 0 1 0 2 2 2 0 1 1 0 2 2 3 3 2 2\n",
      " 0 2 3 2 2 1 1 0 0 0 0 0 2 2 3 3 3 2 0 2 1 0 2 2 0 2 2 0 1 2 1 3 0 2 0 2 2\n",
      " 2 3 1 2 2 1 1 3 1 3 3 2 2 3 3 3 2 2 3 1 3 2 3 3 2 2 2 3 0 2 0 3 3 1 3 2 2\n",
      " 3 2 2 3 2 3 2 2 3 1 1 1 1 2 3 2 1 2 0 3 2 2 0 2 1 3 0 2 0 2 1 2 3 1 3 2 2\n",
      " 0 0 0 0 1 1 2 2 2 0 2 3 1 3 3 2 3 0 0 0 0 2 3 0 2 1 3 0 3 1 0 3 2 0 2 2 1\n",
      " 3 0 2 1 2 2 0 0 2 2 2 1 0 2 2 2 2 2 3 2 0 0 1 0 3 0 0 0 0 2 2 0 2 0 1 1 2\n",
      " 0 2 3 2 2 3 2 3 1 1 2 3 3 3 1 3 3 2 3 3 2 3 1 3 3 3 2 3 0 3 2 3 2 1 1 3 2\n",
      " 3 3 1 3 3 3 2 3 2 3 2 2 3 3 3 3 0 3 3 3 3 3 3 2 1 2 1 1 2 3 3 2 2 2 2 3 3\n",
      " 2 3 3 2 2 2 3 3 3 3 2 3 3 3 0 3 3 0 2 2 2 2 3 3 0 1 2 2 1 2 3 2 2 1 0 1 2\n",
      " 0 0 3 3 1 0 2 2 2 0 0 2 2 1 2 0 3 2 1 2 2 0 3 3 2 2 1 2 3 0 0 0 2 2 3 2 0\n",
      " 1 0 2 3 1 0 2 3 1 3 2 2 1 2 1 3 1 3 3 2 2 0 0 3 3 2 0 1 3 3 2 2 3 2 1 1 3\n",
      " 3 2 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after epoch #26, Validation Loss= 1.6109, Validation Accuracy= 0.352\n",
      "Validation after epoch #27, Validation Loss= 1.6192, Validation Accuracy= 0.375\n",
      "Validation after epoch #28, Validation Loss= 1.6145, Validation Accuracy= 0.362\n",
      "Validation after epoch #29, Validation Loss= 1.6250, Validation Accuracy= 0.367\n",
      "Validation after epoch #30, Validation Loss= 1.6228, Validation Accuracy= 0.359\n",
      "[1 1 1 2 0 2 0 3 0 2 0 2 0 3 3 1 2 2 0 2 3 2 2 2 0 2 2 0 0 2 3 1 3 3 3 3 3\n",
      " 2 3 3 1 2 0 0 2 3 2 2 3 2 3 2 3 3 0 3 3 2 3 2 0 0 0 0 0 3 0 0 1 0 2 3 1 0\n",
      " 2 2 3 0 3 1 0 0 3 0 3 3 1 3 1 1 0 0 2 2 3 1 2 2 0 2 2 3 2 2 1 2 0 2 3 2 3\n",
      " 2 0 3 0 2 3 3 2 1 3 0 3 2 3 1 2 2 2 1 2 2 2 2 3 1 1 0 1 2 1 0 1 2 0 0 1 3\n",
      " 3 0 2 2 0 2 1 2 0 3 2 2 2 0 2 2 2 2 0 0 3 1 3 1 3 2 0 3 2 3 0 3 0 3 1 2 0\n",
      " 3 3 0 0 2 0 2 0 3 1 0 2 2 2 3 3 0 0 0 0 3 1 2 3 3 3 3 1 2 3 1 2 1 2 2 2 3\n",
      " 3 3 2 2 0 3 1 3 3 0 3 3 3 0 3 0 3 3 1 2 1 2 2 1 3 3 3 2 3 1 1 3 3 3 3 3 3\n",
      " 3 2 2 2 2 3 3 3 3 3 2 2 3 3 3 3 3 0 3 3 2 3 3 0 0 1 3 2 2 2 2 3 2 3 2 2 2\n",
      " 3 2 1 2 2 2 0 0 2 0 0 0 2 0 2 0 1 1 2 0 2 3 0 0 2 2 0 0 0 0 0 2 3 2 0 1 0\n",
      " 3 0 0 3 2 0 0 0 0 0 0 2 0 1 0 0 2 2 2 2 1 0 1 0 3 2 2 3 1 1 0 2 0 3 3 2 2\n",
      " 0 2 3 2 2 1 1 1 0 0 0 0 2 0 3 3 3 2 3 1 1 0 2 2 0 2 2 0 1 2 1 3 0 2 0 2 2\n",
      " 2 3 1 2 2 1 1 1 1 3 2 2 2 3 2 3 2 2 3 1 3 2 3 3 2 2 2 3 0 2 0 3 3 1 3 3 2\n",
      " 3 2 3 3 2 3 2 2 3 1 1 2 1 2 3 2 1 2 0 3 2 2 0 2 1 2 0 2 0 2 1 2 3 1 3 2 2\n",
      " 0 0 0 0 2 2 2 2 2 0 2 3 1 3 3 2 3 0 0 0 0 0 3 0 2 1 3 0 3 1 0 3 2 0 2 2 1\n",
      " 3 0 2 1 2 2 0 2 2 2 2 2 0 2 2 2 2 0 3 2 0 0 1 0 3 0 3 0 0 2 2 0 0 0 3 1 2\n",
      " 0 2 3 2 2 3 2 3 1 3 2 3 3 3 2 3 3 2 3 3 3 3 1 3 3 3 3 3 0 3 2 3 2 1 1 3 2\n",
      " 3 3 1 3 3 3 0 3 2 3 2 2 3 3 3 3 0 3 3 3 3 2 3 2 1 2 3 1 2 3 3 3 2 2 2 3 3\n",
      " 2 1 3 2 2 2 2 3 3 3 2 3 3 3 0 3 3 0 2 2 2 2 1 3 0 1 2 2 1 2 3 2 2 1 0 1 2\n",
      " 0 0 2 3 1 0 2 2 2 0 0 2 2 1 2 0 3 2 1 2 2 0 3 3 2 0 1 2 3 0 0 2 2 2 3 2 0\n",
      " 1 0 2 3 1 0 2 3 1 3 1 2 1 2 1 3 3 3 3 2 2 0 0 3 3 2 0 1 2 3 2 2 3 2 1 1 3\n",
      " 3 2 3 3]\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.37197232\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    print('Session initialized.')\n",
    "\n",
    "    for epoch_step in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(train_data, train_labels)\n",
    "        for data_minibatch, label_minibatch, current_index in batch_gen:\n",
    "            sess.run(train_trigger, feed_dict={X: data_minibatch, Y: label_minibatch, keep_prob: dropout, normalization_switch: True})\n",
    "            #train_loss, train_acc = sess.run([loss_function, accuracy], feed_dict={X: data_minibatch, Y: label_minibatch, keep_prob: 1.0, normalization_switch: False})\n",
    "            #print(\"Training accuracy of batch #{0}\".format(current_index) + \": Loss={:.4f}\".format(train_loss) +\", Accuracy={:.4f}.\".format(train_acc))\n",
    "        loss, acc = sess.run([loss_function, accuracy], feed_dict={X: validation_data, Y: validation_labels, keep_prob: 1.0, normalization_switch: False})\n",
    "        print(\"Validation after epoch #\" + str(epoch_step) + \", Validation Loss= \"+ \"{:.4f}\".format(loss) + \", Validation Accuracy= \" + \"{:.3f}\".format(acc))\n",
    "        if (epoch_step % 5 == 0):\n",
    "            preds = sess.run(tf.argmax(prediction, 1), feed_dict={X:validation_data, Y:validation_labels, keep_prob:1.0, normalization_switch: False})\n",
    "            print(preds)\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data,\n",
    "                                      Y: test_labels,\n",
    "                                      keep_prob: 1.0, normalization_switch: False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_test = sess.run(correct_prediction, feed_dict={X:test_data, Y:test_labels, keep_prob:1.0, normalization_switch: False})\n",
    "predictions_validation = sess.run(correct_prediction, feed_dict={X:validation_data, Y:validation_labels, keep_prob:1.0, normalization_switch: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final:  {'ang': 24, 'hap': 5, 'neu': 97, 'sad': 89}\n",
      "Overall:  {'ang': 84, 'hap': 46, 'neu': 275, 'sad': 173}\n"
     ]
    }
   ],
   "source": [
    "coupled_test = list(zip(predictions_test, onehot_decode(test_labels)))\n",
    "final_dict_test = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "overall_dict_test = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "\n",
    "for couple in coupled_test:\n",
    "    if (couple[0]):\n",
    "        final_dict_test[couple[1]] += 1\n",
    "    overall_dict_test[couple[1]] += 1\n",
    "print(\"Final: \", final_dict_test)\n",
    "print(\"Overall: \", overall_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final:  {'ang': 13, 'hap': 14, 'neu': 143, 'sad': 97}\n",
      "Overall:  {'ang': 48, 'hap': 81, 'neu': 422, 'sad': 193}\n"
     ]
    }
   ],
   "source": [
    "coupled_validation = list(zip(predictions_validation, onehot_decode(validation_labels)))\n",
    "final_dict_validation = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "overall_dict_validation = {'ang':0, 'hap':0, 'neu':0, 'sad':0}\n",
    "\n",
    "for couple in coupled_validation:\n",
    "    if (couple[0]):\n",
    "        final_dict_validation[couple[1]] += 1\n",
    "    overall_dict_validation[couple[1]] += 1\n",
    "print(\"Final: \", final_dict_validation)\n",
    "print(\"Overall: \", overall_dict_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_validation_confusion = sess.run(tf.argmax(prediction, 1), feed_dict={X:validation_data, Y:validation_labels, keep_prob:1.0, normalization_switch: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 0, 2, 0, 3, 0, 2, 0, 2, 0, 3, 3, 1, 2, 2, 0, 2, 3, 2,\n",
       "       2, 2, 0, 2, 2, 0, 0, 2, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 2, 0, 0,\n",
       "       2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 0, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 3,\n",
       "       0, 0, 1, 0, 2, 3, 1, 0, 2, 2, 3, 0, 3, 1, 0, 0, 3, 0, 3, 3, 1, 3,\n",
       "       1, 1, 0, 0, 2, 2, 3, 1, 2, 2, 0, 2, 2, 3, 2, 2, 1, 2, 0, 2, 3, 2,\n",
       "       3, 2, 0, 3, 0, 2, 3, 3, 2, 1, 3, 0, 3, 2, 3, 1, 2, 2, 2, 1, 2, 2,\n",
       "       2, 2, 3, 1, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 3, 3, 0, 2, 2, 0, 2,\n",
       "       1, 2, 0, 3, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 3, 1, 3, 1, 3, 2, 0, 3,\n",
       "       2, 3, 0, 3, 0, 3, 1, 2, 0, 3, 3, 0, 0, 2, 0, 2, 0, 3, 1, 0, 2, 2,\n",
       "       2, 3, 3, 0, 0, 0, 0, 3, 1, 2, 3, 3, 3, 3, 1, 2, 3, 1, 2, 1, 2, 2,\n",
       "       2, 3, 3, 3, 2, 2, 0, 3, 1, 3, 3, 0, 3, 3, 3, 0, 3, 0, 3, 3, 1, 2,\n",
       "       1, 2, 2, 1, 3, 3, 3, 2, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2,\n",
       "       3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 0, 3, 3, 2, 3, 3, 0, 0, 1, 3,\n",
       "       2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 1, 2, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "       2, 0, 2, 0, 1, 1, 2, 0, 2, 3, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 3, 2,\n",
       "       0, 1, 0, 3, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 2, 2,\n",
       "       2, 1, 0, 1, 0, 3, 2, 2, 3, 1, 1, 0, 2, 0, 3, 3, 2, 2, 0, 2, 3, 2,\n",
       "       2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 3, 3, 3, 2, 3, 1, 1, 0, 2, 2, 0, 2,\n",
       "       2, 0, 1, 2, 1, 3, 0, 2, 0, 2, 2, 2, 3, 1, 2, 2, 1, 1, 1, 1, 3, 2,\n",
       "       2, 2, 3, 2, 3, 2, 2, 3, 1, 3, 2, 3, 3, 2, 2, 2, 3, 0, 2, 0, 3, 3,\n",
       "       1, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 2, 1, 2,\n",
       "       0, 3, 2, 2, 0, 2, 1, 2, 0, 2, 0, 2, 1, 2, 3, 1, 3, 2, 2, 0, 0, 0,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 3, 1, 3, 3, 2, 3, 0, 0, 0, 0, 0, 3, 0, 2,\n",
       "       1, 3, 0, 3, 1, 0, 3, 2, 0, 2, 2, 1, 3, 0, 2, 1, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 0, 3, 2, 0, 0, 1, 0, 3, 0, 3, 0, 0, 2, 2, 0,\n",
       "       0, 0, 3, 1, 2, 0, 2, 3, 2, 2, 3, 2, 3, 1, 3, 2, 3, 3, 3, 2, 3, 3,\n",
       "       2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2, 1, 1, 3, 2, 3, 3,\n",
       "       1, 3, 3, 3, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 0, 3, 3, 3, 3, 2, 3, 2,\n",
       "       1, 2, 3, 1, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 1, 3, 2, 2, 2, 2, 3, 3,\n",
       "       3, 2, 3, 3, 3, 0, 3, 3, 0, 2, 2, 2, 2, 1, 3, 0, 1, 2, 2, 1, 2, 3,\n",
       "       2, 2, 1, 0, 1, 2, 0, 0, 2, 3, 1, 0, 2, 2, 2, 0, 0, 2, 2, 1, 2, 0,\n",
       "       3, 2, 1, 2, 2, 0, 3, 3, 2, 0, 1, 2, 3, 0, 0, 2, 2, 2, 3, 2, 0, 1,\n",
       "       0, 2, 3, 1, 0, 2, 3, 1, 3, 1, 2, 1, 2, 1, 3, 3, 3, 3, 2, 2, 0, 0,\n",
       "       3, 3, 2, 0, 1, 2, 3, 2, 2, 3, 2, 1, 1, 3, 3, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_validation_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = [onehot.index(1.0) for onehot in validation_labels]\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13  22 112  15]\n",
      " [  6  14  59  23]\n",
      " [ 23  31 143  58]\n",
      " [  6  14 108  97]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predictions_validation_confusion, true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
